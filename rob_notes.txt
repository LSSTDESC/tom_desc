Rob's workflow
 ---> THIS IS CHANGING AS I USE YAML FILES AND THE COMMAND LINE

  * In the local checked out tree ( $HOME/desc/tom_desc )
     * Edit code
     * Commit and push
  * At /global/cfs/cdirs/m1727/tom/deploy_dev/tom_desc
     * git pull
  * On the spin dev server
     * Run python manage.py makemigrations -n <short name>
     * Fix bugs until that works, editing code in deploy_dev/tom_desc
         ( edit on cori, not the spin server, which doesn't have emacs )
         ---> For an example of providing initial data to a table,
              see elasticc/migrations/0019_classificationmap.py
     * Run python manage.py migrate
     * Panic if that didn't work, try to fix it
     * Run kill -HUP 1 to get new code loaded into the server
     * git status
     * git add any migrations that were created and any edited files
     * Commit and push
   * In the local checked out tree:
     * Git pull

When ready to deploy to production:

  * At /global/cfs/cdirs/m1727/tom/deploy_production/tom_desc
     * git pull
  * On the spin production server
     * Run python manage.py migrate
     * Panic if that didn't just work
     * Run kill -HUP 1
     
     

======================================================================

Resetting migrations

WARNING : This will blow away the database tables for the app!!!

I needed to do this as I wanted to move from PostGIS to G3C, but the old
migrations would have required PostGIS to work.  What I did:

 * python manage.py migrate <app> zero
 * blow away everything in <app>/migrations
 * python manage.py makemigrations <app>
 * python manage.py migrate <app>

======================================================================

Moving users from one TOM to another:

# THIS DOESN'T WORK
# WARNING.  This is hacky.  It's not officially documented.  It may be
# horribly broken.  This is a functionality that really OUGHT to be
# officially documented, but, what can you do.
#
# In fact, it CAN'T work, unless you're lucky, and counting on that is
# very scary.  The auth permissions table points to the django_content_type
# table, but that table isn't owned by auth, and will already be
# populated in the new TOM.  The permissions table in the auth
# module points to rows of that table, but it's entirely possible that
# the same primary keys don't have the same meaning in the new TOM.
# So, there's no safe way to do this.  Very annoying.
# 
# Log into the old TOM, and do:
# 
#   python manage.py dumpdata auth --output /tmp/auth.dump
# 
# Get that file off of the old tom:
# 
#   rancher kubectl cp --namespace desc-tom <pod>:/tmp/auth.dump auth.dump
# 
# Push that file to the new tom:
# 
#   rancher kubectl cp --namespace desc-tom auth.dump <pod>:/tmp/auth.dump
# 
# Log into th enew TOM and run:
# 
#   python manage.py loaddata /tmp/auth.dump
# 
# ...and it doesn't work


To preserve users when doing this:
 
  pg_dump -a -t auth_user > users.sql

Manually edit this to remove the line with the anonymous user.

(I did this in /var/lib/postgresql/data since I could write there.  I
then connected with my hack/admin workload that mounts that directory,
and scp'ed it out.  (The postgres image doesn't have scp in it.))

Then, once the database is set back up, on the pod for the workload:

  psql -U postgres -h tom-rknop-dev-postgres tom_desc -f users.sql

You will still manually need to make groups and permissions and add them
back.

======================================================================

Adding a group permission

  from django.contrib.auth.models import Group, Permission
  perm = Permission.objects.get( codename='elasticc_admin' )
  g = Group.objects.get( name='elasticc_admin' )
  g.permissions.add( perm )

To show permissions, to g.permissions.all()

======================================================================

Mouting postgres

I had trouble with my persistent volume claim because postgres couldn't
write to the directory.  I created a temp container (with just some
linux image) that moutned the same persistent mount, went in there and
did a chown of 101:104 (the postgres user and group in my postgres
image) on the persistent mount.

 
======================================================================

Env vars from the old tom that we might need again:

FINK_GROUP_ID lsstfr-johann
FINK_SERVER 134.158.74.95:24499,
FINK_TOPIC fink_early_sn_candidates_ztf
FINK_USERNAME tom-rknop-dev-postgres-7fc9fb874c-77nj4
GOOGLE_APPLICATION_CREDENTIALS /secrets/GCP_auth_key-pitt_broker_user_project.json
GOOGLE_CLOUD_PROJECT pitt-broker-user-project

There's also the secret

GCP_auth_key-pitt_broker_user_project.json

======================================================================

Making a partial elasticc backup (for testing purposes).

FIRST, have to make temp tables

CREATE TABLE temp_diaobject (LIKE elasticc_diaobject);
CREATE TABLE temp_diasource (LIKE elasticc_diasource);
CREATE TABLE temp_diaforcedsource (LIKE elasticc_diaforcedsource);
CREATE TABLE temp_diaalert (LIKE elasticc_diaalert);
CREATE TABLE temp_diaobjecttruth (LIKE elasticc_diaobjecttruth);
CREATE TABLE temp_diatruth (LIKE elasticc_diatruth);
CREATE TABLE temp_brokermessage (LIKE elasticc_brokermessage);
CREATE TABLE temp_brokerclassifier (LIKE elasticc_brokerclassifier);
CREATE TABLE temp_brokerclassification (LIKE elasticc_brokerclassification);

Copy over some objects

INSERT INTO temp_diaobject SELECT * FROM elasticc_diaobject
   ORDER BY RANDOM() LIMIT 1000;
INSERT INTO temp_diasource SELECT * FROM elasticc_diasource
   WHERE "diaObjectId" IN
     ( SELECT "diaObjectId" FROM temp_diaobject );
INSERT INTO temp_diaforcedsource SELECT * FROM elasticc_diaforcedsource
   WHERE "diaObjectId" IN
     ( SELECT "diaObjectId" FROM temp_diaobject );
INSERT INTO temp_diaalert SELECT * FROM elasticc_diaalert
   WHERE "diaObjectId" IN
     ( SELECT "diaObjectId" FROM temp_diaobject );
INSERT INTO temp_diaobjecttruth SELECT * FROM elasticc_diaobjecttruth
   WHERE "diaObjectId" IN
     ( SELECT "diaObjectId" FROM temp_diaobject );
INSERT INTO temp_diatruth SELECT * FROM elasticc_diatruth
  WHERE "diaObjectId" IN
     ( SELECT "diaObjectId" FROM temp_diaobject );
INSERT INTO temp_brokerclassifier SELECT * FROM elasticc_brokerclassifier;
INSERT INTO temp_brokermessage SELECT * FROM elasticc_brokermessage
  WHERE "diaSourceId" IN
     ( SELECT "diaSourceId" FROM temp_diasource );
INSERT INTO temp_brokerclassification
  SELECT * FROM elasticc_brokerclassification
  WHERE "brokerMessageId" IN
     ( SELECT "brokerMessageId" from temp_brokermessage );

Then dump out the temp tables

  pg_dump -h <server> -U postgres --data-only -f elasticc_sample.sql
     -t temp_diaobject -t temp_diasource -t temp_diaforcedsource \
     -t temp_diaobjecttruth -t temp_diatruth \
     -t temp_diaalert -t temp_brokerclassifier \
     -t temp_brokermessage -t temp_brokerclassification \
     tom_desc 

Delete the temp tables:

DROP TABLE temp_diaobject;
DROP TABLE temp_diasource;
DROP TABLE temp_diaforcedsource;
DROP TABLE temp_diaalert;
DROP TABLE temp_diaobjecttruth;
DROP TABLE temp_diatruth;
DROP TABLE temp_message;
DROP TABLE temp_brokerclassifier;
DROP TABLE temp_brokerclassification;


Now the painful part.  First, pg_dump does *not* dump the tables in the
order you requested, so even though I listed them in an order above that
would allow restoration without complaints about missing foreign keys,
the dump file won't have that.  So, you have to emacs the enormous dump
file and make some edits:

* Rename all temp_* tables to elasticc_*
* Order the data for the tables in the order

     elasticc_diaobject
     elasticc_diasource
     elasticc_diaforcedsource
     elasticc_diaalert
     elasticc_diaobjecttruth
     elasticc_diatruth
     elasticc_brokerclassifier
     elasticc_brokermessage
     elasticc_brokerclassification

* Add to the bottom a fix to the three auto sequence that these tables
  use. (NOTE : look at the sequences actually created in the django
  migration to make sure these names are right!!!!!)

SELECT setval(public.'"elasticc_brokerclassification_classificationId_seq"',
              ( SELECT 1+MAX("classificationId") FROM public.elasticc_brokerclassification ) );
SELECT setval(public.'"elasticc_brokerclassifier_dbClassifierIndex_seq"',
              ( SELECT 1+MAX("classifierId") FROM public.elasticc_brokerclassifier ) );
SELECT setval(public.'"elasticc_brokermessage_dbMessageIndex_seq"',
              ( SELECT 1+MAX("brokerMessageId") FROM public.elasticc_brokermessage ) );
