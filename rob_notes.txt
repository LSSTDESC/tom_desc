Rob's workflow

  * In the local checked out tree ( $HOME/desc/tom_desc )
     * Edit code
     * Commit and push
  * At /global/cfs/cdirs/m1727/tom/deploy_dev/tom_desc
     * git pull
  * On the spin dev server
     * Run python manage.py makemigrations -n <short name>
     * Fix bugs until that works, editing code in deploy_dev/tom_desc
         ( edit on cori, not the spin server, which doesn't have emacs )
         ---> For an example of providing initial data to a table,
              see elasticc/migrations/0019_classificationmap.py
     * Run python manage.py migrate
     * Panic if that didn't work, try to fix it
     * Run kill -HUP 1 to get new code loaded into the server
     * git status
     * git add any migrations that were created and any edited files
     * Commit and push
   * In the local checked out tree:
     * Git pull

When ready to deploy to production:

  * At /global/cfs/cdirs/m1727/tom/deploy_production/tom_desc
     * git pull
  * On the spin production server
     * Run python manage.py migrate
     * Panic if that didn't just work
     * Run kill -HUP 1
     
     

======================================================================

Resetting migrations

WARNING : This will blow away the database tables for the app!!!

I needed to do this as I wanted to move from PostGIS to G3C, but the old
migrations would have required PostGIS to work.  What I did:

 * python manage.py migrate <app> zero
 * blow away everything in <app>/migrations
 * python manage.py makemigrations <app>
 * python manage.py migrate <app>

======================================================================

Moving users from one TOM to another:

# THIS DOESN'T WORK
# WARNING.  This is hacky.  It's not officially documented.  It may be
# horribly broken.  This is a functionality that really OUGHT to be
# officially documented, but, what can you do.
#
# In fact, it CAN'T work, unless you're lucky, and counting on that is
# very scary.  The auth permissions table points to the django_content_type
# table, but that table isn't owned by auth, and will already be
# populated in the new TOM.  The permissions table in the auth
# module points to rows of that table, but it's entirely possible that
# the same primary keys don't have the same meaning in the new TOM.
# So, there's no safe way to do this.  Very annoying.
# 
# Log into the old TOM, and do:
# 
#   python manage.py dumpdata auth --output /tmp/auth.dump
# 
# Get that file off of the old tom:
# 
#   rancher kubectl cp --namespace desc-tom <pod>:/tmp/auth.dump auth.dump
# 
# Push that file to the new tom:
# 
#   rancher kubectl cp --namespace desc-tom auth.dump <pod>:/tmp/auth.dump
# 
# Log into th enew TOM and run:
# 
#   python manage.py loaddata /tmp/auth.dump
# 
# ...and it doesn't work


To preserve users when doing this:
 
  pg_dump -a -t auth_user > users.sql

Manually edit this to remove the line with the anonymous user.

(I did this in /var/lib/postgresql/data since I could write there.  I
then connected with my hack/admin workload that mounts that directory,
and scp'ed it out.  (The postgres image doesn't have scp in it.))

Then, once the database is set back up, on the pod for the workload:

  psql -U postgres -h tom-rknop-dev-postgres tom_desc -f users.sql

You will still manually need to make groups and permissions and add them
back.

======================================================================

Adding a group permission

  from django.contrib.auth.models import Group, Permission
  perm = Permission.objects.get( codename='elasticc_admin' )
  g = Group.objects.get( name='elasticc_admin' )
  g.permissions.add( perm )

To show permissions, to g.permissions.all()

======================================================================

Mouting postgres

I had trouble with my persistent volume claim because postgres couldn't
write to the directory.  I created a temp container (with just some
linux image) that moutned the same persistent mount, went in there and
did a chown of 101:104 (the postgres user and group in my postgres
image) on the persistent mount.

 
======================================================================

Env vars from the old tom that we might need again:

FINK_GROUP_ID lsstfr-johann
FINK_SERVER 134.158.74.95:24499,
FINK_TOPIC fink_early_sn_candidates_ztf
FINK_USERNAME tom-rknop-dev-postgres-7fc9fb874c-77nj4
GOOGLE_APPLICATION_CREDENTIALS /secrets/GCP_auth_key-pitt_broker_user_project.json
GOOGLE_CLOUD_PROJECT pitt-broker-user-project

There's also the secret

GCP_auth_key-pitt_broker_user_project.json
